{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Background\n",
    "\n",
    "Recall in data structures learning about the different types of tree structures - binary, red black, and splay trees. In tree based modeling, we work off these structures for classification prediction. \n",
    "\n",
    "Tree based machine learning is great because it's incredibly accurate and stable, as well as easy to interpret. Despite being a linear model, tree based models map non-linear relationships well. The general structure is as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Decision Trees\n",
    "\n",
    "Decision trees are a type of supervised learning algorithm used in classification that works for both categorical and continuous input/output variables. This typle of model includes structures with nodes which represent tests on attributes and the end nodes (leaves) of each branch represent class labels. Between these nodes are what we call edges, which represent a 'decision' that separates the data from the previous node based on some criteria. \n",
    "\n",
    "![alt text](https://www.analyticsvidhya.com/wp-content/uploads/2016/04/dt.png \"Logo Title Text 1\")\n",
    "\n",
    "Looks familiar, right? \n",
    "\n",
    "### 2.1 Nodes\n",
    "\n",
    "As mentioned above, nodes are an important part of the structure of Decision Trees. In this section, we'll review different types of nodes.\n",
    "\n",
    "#### 2.1.1 Root Node\n",
    "\n",
    "The root node is the node at the very top. It represents an entire population or sample because it has yet to be divided by any edges. \n",
    "\n",
    "#### 2.1.2 Decision Node\n",
    "\n",
    "Decision Nodes are the nodes that occur between the root node and leaves of your decision tree. It's considered a decision node because it's a resulting node of an edge that then splits once again into either more decision nodes, or the leaves.\n",
    "\n",
    "#### 2.1.3 Leaves/Terminal Nodes\n",
    "\n",
    "As mentioned before, leaves are the final nodes at the bottom of the decision tree that represent a class label in classification. They're also called <i>terminal nodes</i> because more nodes do not split off of them. \n",
    "\n",
    "#### 2.1.4 Parent and Child Nodes\n",
    "\n",
    "A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node.\n",
    "\n",
    "### 2.2 Pros & Cons\n",
    "\n",
    "#### 2.2.1 Pros\n",
    "\n",
    "1. Easy to Understand: Decision tree output is fairly easy to understand since it doesn't require any statistical knowledge to read and interpret them. Its graphical representation is very intuitive and users can easily relate their hypothesis.\n",
    "\n",
    "2. Useful in Data exploration: Decision tree is one of the fastest way to identify most significant variables and relation between two or more variables. With the help of decision trees, we can create new variables / features that has better power to predict target variable. You can refer article (Trick to enhance power of regression model) for one such trick.  It can also be used in data exploration stage. For example, we are working on a problem where we have information available in hundreds of variables, there decision tree will help to identify most significant variable.\n",
    "\n",
    "3. Less data cleaning required: It requires less data cleaning compared to some other modeling techniques. It is not influenced by outliers and missing values to a fair degree.\n",
    "\n",
    "4. Data type is not a constraint: It can handle both numerical and categorical variables.\n",
    "\n",
    "5. Non Parametric Method: Decision tree is considered to be a non-parametric method. This means that decision trees have no assumptions about the space distribution and the classifier structure.\n",
    "\n",
    "#### 2.2.2 Cons\n",
    "\n",
    "1. Over fitting: Over fitting is one of the most practical difficulty for decision tree models. This problem gets solved by setting constraints on model parameters and pruning (discussed in detailed below).\n",
    "\n",
    "2. Not fit for continuous variables: While working with continuous numerical variables, decision tree looses information when it categorizes variables in different categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Decision Trees in R\n",
    "\n",
    "We begin by loading the required libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Pruning Decision Trees\n",
    "\n",
    "Decision Tree pruning is a technique that reduces the size of decision trees by removing sections (nodes) of the tree that provide little power to classify instances. This is great because it reduces the complexity of the final classifier, which results in increased predictive accuracy by reducing overfitting. \n",
    "\n",
    "Ultimately, our aim is to reduce the cross-validation error. First, we index with the smallest complexity parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Random Forests\n",
    "\n",
    "Recall the *ensemble* learning method from previous lectures. Random Forests are an ensemble learning method that can be used for both classification and regression. It works by combining individual decision trees through **bagging** -- preventing the model from overfitting. \n",
    "\n",
    "What's remarkable about random forests is that it's a reliable method for a wide range of prediction tasks without the need to tune much, unlike SVMs which often require a great deal of tuning.\n",
    "\n",
    "### 3.1 Algorithm\n",
    "\n",
    "First, we create many decision trees through bagging. Once completed, we inject randomness into the decision trees by allowing the trees to grow to their maximum sizes, leaving them unpruned. \n",
    "\n",
    "We make sure that each split is based on randomly selected subset of attributes, which reduces the correlation between different trees. \n",
    "\n",
    "Now we get into the random forest by voting on categories by majority. We begin by splitting the training data into K bootstrap samples by drawing samples from training data with replacement. \n",
    "\n",
    "Next, we estimate individual trees t<sub>i</sub> to the samples and have every regression tree predict a value for the unseen data. Lastly, we estimate those predictions with the formula:\n",
    "\n",
    "![alt text](https://github.com/lesley2958/ml-tree-modeling/blob/master/rf-pred.png?raw=true \"Logo Title Text 1\")\n",
    "\n",
    "where y&#770; is the response vector and x = [x<sub>1</sub>,...,x<sub>N</sub>]<sup>T</sup> &isin; X as the input parameters. \n",
    "\n",
    "\n",
    "### 3.2 Advantages\n",
    "\n",
    "Random Forests allow us to learn non-linearity with a simple algorithm and good performance. It's also a fast training algorithm and resistant to overfitting.\n",
    "\n",
    "What's also phenomenal about Random Forests is that increasing the number of trees decreases the variance without increasing the bias, so the worry of the variance-bias tradeoff isn't as present. \n",
    "\n",
    "The averaging portion of the algorithm also allows the real structure of the data to reveal. Lastly, the noisy signals of individual trees cancel out. \n",
    "\n",
    "### 3.3 Limitations \n",
    "\n",
    "Unfortunately, random forests have high memory consumption because of the many tree constructions. There's also little performance gain from larger training datasets. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
